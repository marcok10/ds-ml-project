{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prepro = pd.read_csv('data/data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train_prepro['target']\n",
    "X = df_train_prepro.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb556c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = ['duration','dep_temp', 'dep_precip', 'dep_wind', 'arr_temp',\n",
    "       'arr_precip', 'arr_wind', 'holiday_length', 'num_passenger_year', \n",
    "       'distance_km', 'expected_duration', 'delay_relative_to_expected', \n",
    "       'duration_ratio', 'dep_lat', 'dep_long', 'arr_lat', 'arr_long']\n",
    "cat_col = ['departure_point', 'arrival_point', 'flight_status', 'aircraft_code','dep_hour',\n",
    "       'dep_day', 'dep_month', 'dep_dayofweek', 'dep_quarter', 'dep_season',\n",
    "       'dep_is_weekend', 'dep_time_of_day', 'arr_hour', 'arr_day', 'arr_month',\n",
    "       'arr_dayofweek', 'arr_quarter', 'arr_season', 'arr_is_weekend',\n",
    "       'arr_time_of_day', 'route', 'is_holiday', 'Country', 'City', 'aircraft_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sparse output for OneHotEncoder to save memory\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform categorical columns (sparse matrix)\n",
    "X_cat_sparse = encoder.fit_transform(X[cat_col])\n",
    "\n",
    "# Scale only the numerical columns and convert to float32\n",
    "X_num_scaled = scaler.fit_transform(X[num_col]).astype(np.float32)\n",
    "\n",
    "# Convert sparse matrix to float32 and combine with numerical features\n",
    "X_encoded_scaled = sparse.hstack([X_num_scaled, X_cat_sparse.astype(np.float32)]).tocsr()\n",
    "\n",
    "# Split the encoded and scaled data\n",
    "X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(\n",
    "    X_encoded_scaled, y, stratify=y, test_size=0.2, random_state=RSEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for the split\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(X)), stratify=y, test_size=0.2, random_state=RSEED\n",
    ")\n",
    "\n",
    "X_train_1_raw = X.iloc[train_idx]\n",
    "X_train_2_raw = X.iloc[test_idx]\n",
    "y_train_1_raw = y.iloc[train_idx]\n",
    "y_train_2_raw = y.iloc[test_idx]\n",
    "\n",
    "# Get categorical column indices for CatBoost\n",
    "cat_features_idx = [X.columns.get_loc(col) for col in cat_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base models - HYPE\n",
    "random_forest_hype = RandomForestRegressor(max_depth=30, \n",
    "                                      max_features='auto', \n",
    "                                      min_samples_leaf=2,\n",
    "                                      min_samples_split=20, \n",
    "                                      n_estimators=300, \n",
    "                                      random_state=RSEED)\n",
    "xgb_hype = XGBRegressor(objective='reg:squarederror',  \n",
    "                   colsample_bytree=0.5111, \n",
    "                   gamma=3.6609, \n",
    "                   learning_rate=0.0583, \n",
    "                   max_depth=10, \n",
    "                   n_estimators=266, \n",
    "                   reg_lambda=9.6965, \n",
    "                   subsample=0.5241,\n",
    "                   random_state=RSEED)\n",
    "ridge_hype = Ridge(alpha=1.5, \n",
    "                   random_state=RSEED, \n",
    "                   solver=\"sag\")\n",
    "knn_hype = KNeighborsRegressor(weights='distance', \n",
    "                               p=1, \n",
    "                               n_neighbors=28)\n",
    "lgbm_hype = LGBMRegressor(subsample=0.8, \n",
    "                     reg_lambda=1.0, \n",
    "                     reg_alpha=1.0, \n",
    "                     num_leaves=63, \n",
    "                     n_estimators=300, \n",
    "                     max_depth=-1, \n",
    "                     learning_rate=0.05, \n",
    "                     colsample_bytree=1.0,\n",
    "                     random_state=RSEED)\n",
    "catboost_hype = CatBoostRegressor(random_strength=10, \n",
    "                             learning_rate=0.1, \n",
    "                             l2_leaf_reg=9, \n",
    "                             iterations=500, \n",
    "                             depth=8, \n",
    "                             border_count=64, \n",
    "                             bagging_temperature=0.5,\n",
    "                             random_state=RSEED)\n",
    "adaboost_hype = AdaBoostRegressor(estimator=XGBRegressor(max_depth=5),\n",
    "                                  learning_rate=0.1,\n",
    "                                  n_estimators=50,\n",
    "                                  random_state=RSEED)\n",
    "gbr_hype = GradientBoostingRegressor(n_estimators=200, \n",
    "                                learning_rate=0.1, \n",
    "                                max_depth=7,\n",
    "                                subsample=0.8, \n",
    "                                random_state=RSEED)\n",
    "svr_hype = SVR(C=41.54172090104322, \n",
    "               epsilon=0.001,\n",
    "               gamma=0.0020588729828704562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87153937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-model to combine all the base models - HYPE\n",
    "meta_xgb_hype = XGBRegressor(objective='reg:squarederror', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Random Forest\n",
    "rf_model_hype = random_forest_hype.fit(X_train_1, y_train_1) \n",
    "#Fit XGBoost\n",
    "xgb_model_hype = xgb_hype.fit(X_train_1, y_train_1)\n",
    "# Fit Ridge Convert sparse matrix to dense for Ridge regression\n",
    "ridge_model_hype = ridge_hype.fit(X_train_1.toarray(), y_train_1)\n",
    "# fit KNN KNeighborsRegressor does not support sparse input, so convert to dense\n",
    "knn_model_hype = knn_hype.fit(X_train_1, y_train_1)\n",
    "# Fit LightGBM\n",
    "lgbm_model_hype = lgbm_hype.fit(X_train_1, y_train_1)\n",
    "# Fit CatBoost\n",
    "# Drop datetime columns that CatBoost cannot handle\n",
    "datetime_cols = ['departure_time', 'arrival_time', 'departure_date', 'arrival_date']\n",
    "X_train_1_raw_catboost = X_train_1_raw.drop(columns=datetime_cols)\n",
    "\n",
    "# Update categorical feature indices for the new dataframe\n",
    "cat_features_idx_catboost = [X_train_1_raw_catboost.columns.get_loc(col) for col in cat_col if col in X_train_1_raw_catboost.columns]\n",
    "\n",
    "catboost_model_hype = catboost_hype.fit(X_train_1_raw_catboost, y_train_1_raw, cat_features=cat_features_idx_catboost)\n",
    "# Fit AdaBoost on encoded/scaled training data\n",
    "adaboost_model_hype = adaboost_hype.fit(X_train_1, y_train_1)\n",
    "# Fit GradientBoostingRegressor on encoded/scaled training data\n",
    "gbr_model_hype = gbr_hype.fit(X_train_1, y_train_1)\n",
    "# Fit support vector regression - SVR does not support sparse input, so convert to dense\n",
    "svr_model_hype = svr_hype.fit(X_train_1.toarray(), y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a36e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_hype = rf_model_hype.predict(X_train_2)\n",
    "xgb_pred_hype = xgb_model_hype.predict(X_train_2)\n",
    "ridge_pred_hype = ridge_model_hype.predict(X_train_2)\n",
    "knn_pred_hype = knn_model_hype.predict(X_train_2)\n",
    "lgbm_pred_hype = lgbm_model_hype.predict(X_train_2)\n",
    "catboost_pred_hype = catboost_model_hype.predict(X_train_2_raw.drop(columns=datetime_cols))\n",
    "adaboost_pred_hype = adaboost_model_hype.predict(X_train_2)\n",
    "gbr_pred_hype = gbr_model_hype.predict(X_train_2)\n",
    "svr_pred_hype = svr_model_hype.predict(X_train_2.toarray())\n",
    "\n",
    "# Combine base model predictions for meta-model input\n",
    "combine_X_pred_test_hype = pd.concat([\n",
    "\tpd.DataFrame(rf_pred_hype),\n",
    "\tpd.DataFrame(xgb_pred_hype),\n",
    "\tpd.DataFrame(ridge_pred_hype),\n",
    "    pd.DataFrame(knn_pred_hype),\n",
    "    pd.DataFrame(lgbm_pred_hype),\n",
    "    pd.DataFrame(catboost_pred_hype),\n",
    "    pd.DataFrame(adaboost_pred_hype),\n",
    "    pd.DataFrame(gbr_pred_hype),\n",
    "    pd.DataFrame(svr_pred_hype)\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure unique column names for stacking features\n",
    "combine_X_pred_test_hype.columns = [f'hype_model_{i}' for i in range(combine_X_pred_test_hype.shape[1])]\n",
    "\n",
    "meta_xgb_hype.fit(combine_X_pred_test_hype, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hype = meta_xgb_hype.predict(combine_X_pred_test_hype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure predictions are non-negative\n",
    "y_pred_hype[y_pred_hype < 0] = 0\n",
    "y_train_2 = y_train_2.clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779775a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_hype = mean_squared_error(y_train_2, y_pred_hype)\n",
    "r2_hype = r2_score(y_train_2, y_pred_hype)\n",
    "rmse_hype = np.sqrt(mean_squared_error(y_train_2, y_pred_hype))\n",
    "print(f'Hype Mean Squared Error: {mse_hype}')\n",
    "print(f'Hype R2 Score: {r2_hype}')\n",
    "print(f\"Hype Stacking RMSE: {rmse_hype:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7de37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_hype[rf_pred_hype < 0] = 0\n",
    "xgb_pred_hype[xgb_pred_hype < 0] = 0\n",
    "ridge_pred_hype[ridge_pred_hype < 0] = 0\n",
    "knn_pred_hype[knn_pred_hype < 0] = 0\n",
    "lgbm_pred_hype[lgbm_pred_hype < 0] = 0\n",
    "catboost_pred_hype[catboost_pred_hype < 0] = 0\n",
    "adaboost_pred_hype[adaboost_pred_hype < 0] = 0\n",
    "gbr_pred_hype[gbr_pred_hype < 0] = 0\n",
    "svr_pred_hype[svr_pred_hype < 0] = 0\n",
    "\n",
    "print(f\"Hype RF RMSE: {np.sqrt(mean_squared_error(y_train_2, rf_pred_hype))}\")\n",
    "print(f\"Hype XGB RMSE: {np.sqrt(mean_squared_error(y_train_2, xgb_pred_hype))}\")\n",
    "print(f\"Hype Ridge RMSE: {np.sqrt(mean_squared_error(y_train_2, ridge_pred_hype))}\")\n",
    "print(f\"Hype KNN RMSE: {np.sqrt(mean_squared_error(y_train_2, knn_pred_hype))}\")\n",
    "print(f\"Hype LGBM RMSE: {np.sqrt(mean_squared_error(y_train_2, lgbm_pred_hype))}\")\n",
    "print(f\"Hype CatBoost RMSE: {np.sqrt(mean_squared_error(y_train_2, catboost_pred_hype))}\")\n",
    "print(f\"Hype AdaBoost RMSE: {np.sqrt(mean_squared_error(y_train_2, adaboost_pred_hype))}\")\n",
    "print(f\"Hype GBR RMSE: {np.sqrt(mean_squared_error(y_train_2, gbr_pred_hype))}\")\n",
    "print(f\"Hype SVR RMSE: {np.sqrt(mean_squared_error(y_train_2, svr_pred_hype))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_df = pd.DataFrame(X_encoded_scaled.toarray(), columns=encoder.get_feature_names_out(cat_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38119cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_data = pd.read_csv('data/encoded_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba6495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to numpy array, then to sparse CSR matrix\n",
    "X_encoded_test_sparse = sparse.csr_matrix(encoded_test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_df = pd.DataFrame(X_encoded_test_sparse.toarray(), columns=encoded_test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922687c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure test DataFrame has same columns as train DataFrame after one-hot encoding\n",
    "\n",
    "def align_test_to_train(train_df, test_df):\n",
    "    # Add missing columns to test_df, fill with 0\n",
    "    missing_cols = set(train_df.columns) - set(test_df.columns)\n",
    "    for col in missing_cols:\n",
    "        test_df[col] = 0\n",
    "\n",
    "    # Remove extra columns from test_df\n",
    "    extra_cols = set(test_df.columns) - set(train_df.columns)\n",
    "    test_df = test_df.drop(columns=extra_cols)\n",
    "\n",
    "    # Reorder columns to match train_df\n",
    "    test_df = test_df[train_df.columns]\n",
    "    return test_df\n",
    "\n",
    "# Convert sparse matrices to DataFrames using correct column names\n",
    "encoded_cat_cols = encoder.get_feature_names_out(cat_col)\n",
    "all_feature_names = num_col + list(encoded_cat_cols)\n",
    "\n",
    "# Convert sparse matrices to DataFrames using correct column names\n",
    "train_df = pd.DataFrame(X_encoded_scaled.toarray(), columns=all_feature_names)\n",
    "test_df = pd.DataFrame(X_encoded_test_sparse.toarray(), columns=encoded_test_data.columns)\n",
    "\n",
    "aligned_test_df = align_test_to_train(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare raw test data for CatBoost prediction\n",
    "# 1. Load your raw test data (not encoded)\n",
    "raw_test_data = pd.read_csv('data/data_test.csv')  # Update path/filename as needed\n",
    "\n",
    "# 2. Drop datetime columns that CatBoost cannot handle\n",
    "datetime_cols = ['departure_time', 'arrival_time', 'departure_date', 'arrival_date']\n",
    "raw_test_data_catboost = raw_test_data.drop(columns=datetime_cols)\n",
    "\n",
    "# 3. Get categorical feature indices for CatBoost\n",
    "cat_features_idx_catboost = [raw_test_data_catboost.columns.get_loc(col) for col in cat_col if col in raw_test_data_catboost.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1074448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure test columns match training columns exactly\n",
    "raw_test_data_catboost = raw_test_data_catboost[X_train_1_raw_catboost.columns]\n",
    "\n",
    "# Ensure categorical columns are string type\n",
    "for col in cat_col:\n",
    "    if col in raw_test_data_catboost.columns:\n",
    "        raw_test_data_catboost[col] = raw_test_data_catboost[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate base model predictions for the test set\n",
    "rf_pred_test = rf_model_hype.predict(aligned_test_df)\n",
    "xgb_pred_test = xgb_model_hype.predict(aligned_test_df)\n",
    "ridge_pred_test = ridge_model_hype.predict(aligned_test_df.to_numpy())\n",
    "knn_pred_test = knn_model_hype.predict(aligned_test_df)\n",
    "lgbm_pred_test = lgbm_model_hype.predict(aligned_test_df)\n",
    "catboost_pred_test = catboost_model_hype.predict(raw_test_data_catboost)\n",
    "adaboost_pred_test = adaboost_model_hype.predict(aligned_test_df)\n",
    "gbr_pred_test = gbr_model_hype.predict(aligned_test_df)\n",
    "svr_pred_test = svr_model_hype.predict(aligned_test_df.to_numpy())\n",
    "\n",
    "# Combine predictions into a DataFrame with correct column names\n",
    "combine_X_pred_test = pd.DataFrame({\n",
    "\t'hype_model_0': rf_pred_test,\n",
    "\t'hype_model_1': xgb_pred_test,\n",
    "\t'hype_model_2': ridge_pred_test,\n",
    "\t'hype_model_3': knn_pred_test,\n",
    "\t'hype_model_4': lgbm_pred_test,\n",
    "\t'hype_model_5': catboost_pred_test,\n",
    "\t'hype_model_5': adaboost_pred_test,\n",
    "\t'hype_model_6': gbr_pred_test,\n",
    "\t'hype_model_7': svr_pred_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure combine_X_pred_test has all 9 base model predictions with unique column names\n",
    "combine_X_pred_test = pd.DataFrame({\n",
    "\t'hype_model_0': rf_pred_test,\n",
    "\t'hype_model_1': xgb_pred_test,\n",
    "\t'hype_model_2': ridge_pred_test,\n",
    "\t'hype_model_3': knn_pred_test,\n",
    "\t'hype_model_4': lgbm_pred_test,\n",
    "\t'hype_model_5': catboost_pred_test,  # CatBoost predictions (raw_test_data_catboost)\n",
    "\t'hype_model_6': adaboost_pred_test,\n",
    "\t'hype_model_7': gbr_pred_test,\n",
    "\t'hype_model_8': svr_pred_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95827625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate base model predictions for the test set\n",
    "rf_pred_test = rf_model_hype.predict(aligned_test_df)\n",
    "xgb_pred_test = xgb_model_hype.predict(aligned_test_df)\n",
    "ridge_pred_test = ridge_model_hype.predict(aligned_test_df.to_numpy())\n",
    "knn_pred_test = knn_model_hype.predict(aligned_test_df)\n",
    "lgbm_pred_test = lgbm_model_hype.predict(aligned_test_df)\n",
    "catboost_pred_test = catboost_model_hype.predict(raw_test_data_catboost, cat_features=cat_features_idx_catboost)\n",
    "# CatBoost expects raw features, not encoded; skip or use raw test if available\n",
    "adaboost_pred_test = adaboost_model_hype.predict(aligned_test_df)\n",
    "gbr_pred_test = gbr_model_hype.predict(aligned_test_df)\n",
    "svr_pred_test = svr_model_hype.predict(aligned_test_df.to_numpy())\n",
    "\n",
    "# Combine predictions into a DataFrame with correct column names\n",
    "combine_X_pred_test = pd.DataFrame({\n",
    "\t'hype_model_0': rf_pred_test,\n",
    "\t'hype_model_1': xgb_pred_test,\n",
    "\t'hype_model_2': ridge_pred_test,\n",
    "\t'hype_model_3': knn_pred_test,\n",
    "\t'hype_model_4': lgbm_pred_test,\n",
    "\t'hype_model_5': catboost_pred_test, # Only if you have raw test data for CatBoost\n",
    "\t'hype_model_5': adaboost_pred_test,\n",
    "\t'hype_model_6': gbr_pred_test,\n",
    "\t'hype_model_7': svr_pred_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the meta-model\n",
    "y_pred_test = meta_xgb_hype.predict(combine_X_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef67b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure predictions are non-negative\n",
    "y_pred_test[y_pred_test < 0] = 0\n",
    "y_train_2 = y_train_2.clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_df = pd.DataFrame(y_pred_test, columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3fb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest feature importances\n",
    "rf_importances = rf_model_hype.feature_importances_\n",
    "\n",
    "# XGBoost feature importances\n",
    "xgb_importances = xgb_model_hype.feature_importances_\n",
    "\n",
    "# KNN does not provide feature importances, so skip it\n",
    "\n",
    "# Ridge feature coefficients (absolute value for importance)\n",
    "ridge_importances = np.abs(ridge_model_hype.coef_)\n",
    "\n",
    "# LightGBM feature importances\n",
    "lgbm_importances = lgbm_model_hype.feature_importances_\n",
    "\n",
    "# CatBoost feature importances\n",
    "catboost_importances = catboost_hype.get_feature_importance()\n",
    "\n",
    "# AdaBoost feature importances (using the base estimator's feature importances)\n",
    "adaboost_importances = adaboost_model_hype.feature_importances_\n",
    "\n",
    "# Gradient Boosting feature importances\n",
    "gbr_importances = gbr_model_hype.feature_importances_\n",
    "\n",
    "# Support Vector Regression does not provide feature importances, so skip it\n",
    "\n",
    "# Get encoded categorical column names\n",
    "encoded_cat_cols = encoder.get_feature_names_out(cat_col)\n",
    "\n",
    "# Feature names\n",
    "feature_names = num_col + list(encoded_cat_cols)\n",
    "\n",
    "# Create a DataFrame for each model's importances\n",
    "importances_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'RandomForest': rf_importances,\n",
    "    'XGBoost': xgb_importances,\n",
    "    'Ridge': ridge_importances,\n",
    "    'LightGBM': lgbm_importances,\n",
    "    #'CatBoost': catboost_importances,\n",
    "    'AdaBoost': adaboost_importances,\n",
    "    'GradientBoosting': gbr_importances\n",
    "})\n",
    "\n",
    "# Show top 15 features by average importance across models\n",
    "importances_df['avg_importance'] = importances_df[[\n",
    "    'RandomForest',\n",
    "    'XGBoost', \n",
    "    'Ridge', \n",
    "    'LightGBM',\n",
    "    #'CatBoost',\n",
    "    'AdaBoost',\n",
    "    'GradientBoosting'\n",
    "    ]].mean(axis=1)\n",
    "importances_df.sort_values('avg_importance', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
