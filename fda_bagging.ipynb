{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load dataset\n",
    "df = pd.read_csv('data_new/preprocessed_train_data_with_date_hol_concat.csv')\n",
    "df.columns\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71368454",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target']\n",
    "X = df.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# Define the categorical features\n",
    "num_col = ['duration','dep_temp', 'dep_precip', 'dep_wind', 'arr_temp',\n",
    "       'arr_precip', 'arr_wind', 'holiday_length']\n",
    "cat_col = ['departure_point', 'arrival_point', 'flight_status', 'aircraft_code','dep_hour',\n",
    "       'dep_day', 'dep_month', 'dep_dayofweek', 'dep_quarter', 'dep_season',\n",
    "       'dep_is_weekend', 'dep_time_of_day', 'arr_hour', 'arr_day', 'arr_month',\n",
    "       'arr_dayofweek', 'arr_quarter', 'arr_season', 'arr_is_weekend',\n",
    "       'arr_time_of_day', 'route', 'is_holiday', 'Country', 'City']\n",
    "\n",
    "\n",
    "\n",
    "# Use sparse output for OneHotEncoder to save memory\n",
    "encoder = OneHotEncoder(sparse=True, handle_unknown='ignore')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform categorical columns (sparse matrix)\n",
    "X_cat_sparse = encoder.fit_transform(X[cat_col])\n",
    "\n",
    "# Scale only the numerical columns and convert to float32\n",
    "X_num_scaled = scaler.fit_transform(X[num_col]).astype(np.float32)\n",
    "\n",
    "# Convert sparse matrix to float32 and combine with numerical features\n",
    "from scipy import sparse\n",
    "X_encoded_scaled = sparse.hstack([X_num_scaled, X_cat_sparse.astype(np.float32)]).tocsr()\n",
    "\n",
    "# Split the encoded and scaled data\n",
    "X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(\n",
    "    X_encoded_scaled, y, stratify=y, test_size=0.2, random_state=RSEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "904c503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor MSE: 2249.129\n",
      "Boosting Regressor MSE: 2388.588\n"
     ]
    }
   ],
   "source": [
    "# 2. Train-test split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', num_col),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_col)\n",
    "    ])\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# BAGGING REGRESSOR\n",
    "# -------------------\n",
    "bagging_regressor = BaggingRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bagging_regressor_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', bagging_regressor)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "bagging_regressor_pipeline.fit(X_train, y_train)\n",
    "bagging_preds = bagging_regressor_pipeline.predict(X_test)\n",
    "bagging_mse = mean_squared_error(y_test, bagging_preds)\n",
    "print(f\"Bagging Regressor MSE: {bagging_mse:.3f}\")\n",
    "\n",
    "# -------------------\n",
    "# BOOSTING REGRESSOR\n",
    "# -------------------\n",
    "boosting_regressor = GradientBoostingRegressor(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "boosting_regressor_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', boosting_regressor)\n",
    "])\n",
    "boosting_regressor_pipeline.fit(X_train, y_train)\n",
    "boosting_preds = boosting_regressor_pipeline.predict(X_test)\n",
    "boosting_mse = mean_squared_error(y_test, boosting_preds)\n",
    "print(f\"Boosting Regressor MSE: {boosting_mse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bbb812d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting RMSE: 48.87\n",
      "Begging RMSE: 47.42\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, boosting_preds))\n",
    "print(f\"Boosting RMSE: {rmse:.2f}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, bagging_preds))\n",
    "print(f\"Begging RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc99950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mahesh\\Spiced\\water_ml_ops\\ds-ml-project\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor(XGBRegressor) MSE: 2467.222\n",
      "Begging (XGBRegressor) RMSE: 49.67\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# BAGGING REGRESSOR\n",
    "# -------------------\n",
    "bagging_regressor = BaggingRegressor(\n",
    "        base_estimator=XGBRegressor(\n",
    "            n_estimators=10,\n",
    "            learning_rate=0.2,\n",
    "            max_depth=3,\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42\n",
    "        ),\n",
    "        n_estimators=10,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "bagging_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('regressor', bagging_regressor)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "bagging_pipeline.fit(X_train, y_train)\n",
    "bagging_preds = bagging_pipeline.predict(X_test)\n",
    "bagging_mse = mean_squared_error(y_test, bagging_preds)\n",
    "print(f\"Bagging Regressor(XGBRegressor) MSE: {bagging_mse:.3f}\")\n",
    "rmse = np.sqrt(mean_squared_error(y_test, bagging_preds))\n",
    "print(f\"Begging (XGBRegressor) RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d673b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting Regressor(XGBRegressor) MSE: 2342.433\n",
      "Begging (XGBRegressor) RMSE: 48.40\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# BOOSTING REGRESSOR with XGBRegressor\n",
    "# -------------------\n",
    "boosting_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('regressor', XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "boosting_pipeline.fit(X_train, y_train)\n",
    "boosting_preds = boosting_pipeline.predict(X_test)\n",
    "boosting_mse = mean_squared_error(y_test, boosting_preds)\n",
    "print(f\"Boosting Regressor(XGBRegressor) MSE: {boosting_mse:.3f}\")\n",
    "rmse = np.sqrt(mean_squared_error(y_test, boosting_preds))\n",
    "print(f\"Begging (XGBRegressor) RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df4f7655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best Boosting MSE: 2292.52\n",
      "Best Boosting Params: {'regressor__learning_rate': 0.1, 'regressor__max_depth': 4, 'regressor__n_estimators': 120, 'regressor__subsample': 0.8}\n",
      "Begging (XGBRegressor) RMSE: 47.88\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "boosting_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('regressor', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "boosting_param_grid = {\n",
    "    'regressor__learning_rate': [0.05, 0.1],\n",
    "    'regressor__subsample': [0.8, 1.0],\n",
    "    'regressor__n_estimators': [80, 100, 120],\n",
    "    'regressor__max_depth': [3, 4],\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "boosting_search = GridSearchCV(\n",
    "    boosting_pipeline,\n",
    "    param_grid=boosting_param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "boosting_search.fit(X_train, y_train)\n",
    "boosting_preds = boosting_search.predict(X_test)\n",
    "boosting_mse = mean_squared_error(y_test, boosting_preds)\n",
    "print(f\"Best Boosting MSE: {boosting_mse:.2f}\")\n",
    "print(\"Best Boosting Params:\", boosting_search.best_params_)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, boosting_preds))\n",
    "print(f\"Begging (XGBRegressor) RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a220c0e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m best_pipeline = \u001b[43mrandom_search\u001b[49m.best_estimator_\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[32m      4\u001b[39m y_pred = best_pipeline.predict(X_test)\n",
      "\u001b[31mNameError\u001b[39m: name 'random_search' is not defined"
     ]
    }
   ],
   "source": [
    "best_pipeline = random_search.best_estimator_\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "y_pred[y_pred < 0] = 0\n",
    "y_test = y_test.clip(lower=0)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Best Params: {random_search.best_params_}\")\n",
    "print(f\"Test RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec65d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical features\n",
    "num_cols = ['duration']\n",
    "cat_cols = ['departure_point', 'arrival_point', 'flight_status', 'aircraft_code']\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define individual regressors\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "ridge = Ridge(alpha=1.0, solver='lsqr')\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Voting Regressor\n",
    "voting_regressor = VotingRegressor(\n",
    "    estimators=[\n",
    "        # ('rf', rf),\n",
    "        ('xgb', xgb),\n",
    "        ('ridge', ridge),\n",
    "        ('knn', knn)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', voting_regressor)\n",
    "])\n",
    "\n",
    "# Train\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Voting Regressor RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define individual models\n",
    "ridge = Ridge(solver='lsqr')\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Create the voting regressor\n",
    "voting = VotingRegressor(estimators=[\n",
    "    # ('ridge', ridge),\n",
    "    # ('gb', gb),\n",
    "    ('xgb', xgb)\n",
    "])\n",
    "\n",
    "# Final pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', voting)\n",
    "])\n",
    "\n",
    "# Hyperparameter space\n",
    "# param_distributions = {\n",
    "#     # 'regressor__ridge__alpha': [0.1, 1.0, 10.0],\n",
    "#     # 'regressor__gb__n_estimators': [100, 200],\n",
    "#     # 'regressor__gb__max_depth': [3, 5, 7],\n",
    "#     # 'regressor__gb__learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'regressor__xgb__n_estimators': [100, 200],\n",
    "#     'regressor__xgb__max_depth': [3, 5, 7],\n",
    "#     'regressor__xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'regressor__xgb__subsample': [0.8, 1.0]\n",
    "# }\n",
    "\n",
    "param_distributions = {\n",
    "    'regressor__xgb__n_estimators': [100, 200, 300],\n",
    "    'regressor__xgb__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__xgb__max_depth': [3, 5, 6, 8],\n",
    "    'regressor__xgb__min_child_weight': [1, 3, 5],\n",
    "    'regressor__xgb__gamma': [0, 0.1, 0.2],\n",
    "    'regressor__xgb__subsample': [0.7, 0.8, 1.0],\n",
    "    'regressor__xgb__colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'regressor__xgb__reg_alpha': [0, 0.1, 0.5],\n",
    "    'regressor__xgb__reg_lambda': [1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Best model and evaluation\n",
    "print(\"Best parameters:\\n\", search.best_params_)\n",
    "y_pred = search.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
